{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tlg83ca6HWp6"
   },
   "source": [
    "\n",
    "# Tutorial: Using the Delta Method and the Method of Composition for uncertainty propagation\n",
    "\n",
    "In this tutorial, we illustrate how to use the **Delta Method** and the **Method of Composition** approaches to propagate uncertainty to downstream tasks using GloVe-V, our word-level variance estimates for GloVe. As an example, we compute uncertainty intervals for the cosine similarity of the words `doctor` and `surgeon` using both approaches. This computation was performed using the Method of Composition in Figure 5 of our paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L1AdgsDEHYcE"
   },
   "source": [
    "\n",
    "\n",
    "## Background\n",
    "\n",
    "Our GloVe-V framework computes the following Normal distribution for word $i$:\n",
    "\n",
    "$$ w_i \\sim N(\\mu_i, \\Sigma_i),$$\n",
    "\n",
    "where $\\mu_i$ is the $d$-dimensional GloVe-trained word embedding for word $i$ and $\\Sigma_i$ is the $d \\times d$ GloVe-V covariance matrix, as given by Equation 6 in the paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Delta Method**\n",
    "\n",
    "The Delta Method states that if $\\sqrt{n}(W - \\hat{W})$ converges to $N(0, \\Sigma)$, then\n",
    "\n",
    "$$ \\sqrt{n}(\\phi(W) - \\phi(\\hat{W})) \\rightarrow N(0, \\phi^{\\prime}(W)^T\\Sigma\\phi^{\\prime}(W)) ,$$  \n",
    "\n",
    "where $\\phi(\\cdot)$ is a differentiable function of $W$ and $\\phi^{\\prime}(\\cdot)$ is its gradient with respect to $W$. \n",
    "\n",
    "In our example, $\\phi(\\cdot)$ is the cosine similarity of the point estimates of the words $j = $ `doctor` and $k=$ `surgeon`:\n",
    "\n",
    "$$\\phi(w_j, w_k) = \\frac{w_j^T w_k}{\\|w_j\\| \\|w_k\\|} $$\n",
    "\n",
    "We now compute $\\frac{\\partial \\cos(w_j, w_k)}{\\partial w_j}$, the derivative of the cosine similarity with respect to one of the vectors, which is symmetrical for  $w_j$ and $w_k$.\n",
    "\n",
    "$$d_j := \\frac{\\partial \\cos(w_j, w_k)}{\\partial w_j} = \\frac{w_k}{\\|w_k\\| \\|w_j\\|} - \\cos(w_j, w_k) \\cdot\\frac{w_j}{\\|\n",
    "    w_j\\|^2}  $$\n",
    "\n",
    "Then, the variance of $\\phi(W)$ is given by:\n",
    "$$ \\text{var}(\\phi(W)) = \\phi^{\\prime}(W)^T\\Sigma\\phi^{\\prime}(W) = \\sum_{i \\in \\{j, k\\}} d_i^T \\Sigma_i d_i$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Method of Composition (Tanner, 1996)**\n",
    "\n",
    "The Method of Composition propagates the uncertainty from a set of input variables to an output variable $Y$, generating independent and identically distributed samples of the output variable. In our example, $Y = \\cos(w_j, w_k)$.\n",
    "\n",
    "Let $K$ be the number of iterations. In the $k$th iteration, we draw one sample from each of the input variables $x_j \\sim N(\\mu_j, \\Sigma_j)$ and $x_k \\sim N(\\mu_k, \\Sigma_k)$, and compute $Y^{(k)} = \\cos(x_j, x_k)$. Then, ($Y^{(1)}, ..., Y^{(K)}$) are i.i.d. from the marginal distribution of $Y$, and we can compute an estimate of the mean and variance of $Y$ as follows:\n",
    "\n",
    "$$\\hat{Y} = \\frac{1}{K} \\sum_k Y^{(k)}$$\n",
    "\n",
    "$$ \\text{var}(\\hat{Y}) = \\frac{1}{K-1} \\sum_k (Y^{(k)} - \\hat{Y})^2 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "dYjIJ7paHUOS"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/avaimar/Documents/Projects/Legal-NLP/glove-v/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Set up environment\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import glove_v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A1LTwygwHlYD"
   },
   "source": [
    "### Download COHA (1900-1999) vectors and pre-computed variances\n",
    "\n",
    "We start by downloading the pre-computed variances for the COHA (1900-1999) corpus. In this example, we download only a small subset which includes the vectors and variances for the words `doctor` and `surgeon`, which we make available in the `Toy-Embeddings` folder. \n",
    "\n",
    "To obtain the vectors and variances for the full vocabulary of the 1900-1999 COHA corpus, you can use `COHA_1900-1999_300d` as the `embedding_name` argument in the `download_embeddings` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_v.data.download_embeddings(\n",
    "    embedding_name=\"Toy-Embeddings\",\n",
    "    approximation=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0LVvMY84N84_"
   },
   "source": [
    "### Load the vocabulary, vectors and variances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "EGyrmAKIIDDO"
   },
   "outputs": [],
   "source": [
    "# Vocabulary and inverse vocabulary\n",
    "vocab, ivocab = glove_v.vector.load_vocab(\n",
    "    embedding_name=\"Toy-Embeddings\",\n",
    ")\n",
    "# Vectors and variances\n",
    "vectors = glove_v.vector.load_vectors(\n",
    "    embedding_name=\"Toy-Embeddings\", format=\"dictionary\"\n",
    ")\n",
    "variances = {}\n",
    "for word in list(vocab.keys()):\n",
    "    variances[word] = glove_v.variance.load_variance(\n",
    "        embedding_name=\"Toy-Embeddings\",\n",
    "        approximation=False,\n",
    "        word_idx=vocab[word],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZeZV6aU3PHwn"
   },
   "source": [
    "We can see that the dictionaries containing the vectors and pre-computed variances include the keys `doctor` and `surgeon`, as well as other occupations used in the generation of Figure 5 in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jTmmTTvYPGmv",
    "outputId": "ca822af6-39de-4e8d-c221-01015d41e72b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys in vectors dictionary: dict_keys(['doctor', 'surgeon', 'dentist', 'psychiatrist', 'therapist', 'veterinarian', 'obstetrician', 'pediatrician', 'pharmacist', 'neurologist', 'gynecologist'])\n",
      "Keys in variances dictionary: dict_keys(['doctor', 'surgeon', 'dentist', 'psychiatrist', 'therapist', 'veterinarian', 'obstetrician', 'pediatrician', 'pharmacist', 'neurologist', 'gynecologist'])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Keys in vectors dictionary: {vectors.keys()}\")\n",
    "print(f\"Keys in variances dictionary: {variances.keys()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ck6zkYFlIKp7"
   },
   "source": [
    "## Cosine similarity point estimate\n",
    "We now compute the point estimate for the cosine similarity between `doctor` and `surgeon` using the GloVe-trained vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "djj_heViIDIf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between \"doctor\" and \"surgeon\": 0.443281888961792\n"
     ]
    }
   ],
   "source": [
    "cs_pe = np.dot(vectors[\"doctor\"], vectors[\"surgeon\"])\n",
    "cs_pe /= np.linalg.norm(vectors[\"doctor\"]) * np.linalg.norm(vectors[\"surgeon\"])\n",
    "print(f'Cosine similarity between \"doctor\" and \"surgeon\": {cs_pe}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JhtFLFfaINFG"
   },
   "source": [
    "## Delta Method approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aGcl9d-NQnVl"
   },
   "source": [
    "We'll start by building a dictionary of derivatives for each word. We use the `cosine_derivative` function in `glove_v.propagate`, which implements the following computation for the derivative of the cosine similarity with respect to one of the vectors:\n",
    "\n",
    "$$d_j := \\frac{\\partial \\cos(w_j, w_k)}{\\partial w_j} = \\frac{w_k}{\\|w_k\\| \\|w_j\\|} - \\cos(w_j, w_k) \\cdot\\frac{w_j}{\\|w_j\\|^2}  $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "3GiUjvcYP9j5"
   },
   "outputs": [],
   "source": [
    "deriv_dict = {}\n",
    "for w in [\"doctor\", \"surgeon\"]:\n",
    "    w_vec = vectors[w]\n",
    "    other_w = \"doctor\" if w == \"surgeon\" else \"surgeon\"\n",
    "    c_vec = vectors[other_w]\n",
    "    w_der = glove_v.propagate.cosine_derivative(u=w_vec, v=c_vec)\n",
    "    deriv_dict[w] = w_der.reshape(1, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tKedczE4RZPa"
   },
   "source": [
    "Next, we compute the variance of the cosine similarity, $\\text{var}(\\phi(W))$, as follows, using the `delta_method_variance` function in `glove_v.propagate`:\n",
    "\n",
    "$$ \\text{var}(\\phi(W)) = \\sum_{i \\in \\{j, k\\}} d_i^T \\Sigma_i d_i$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "T5W-mgFlRD5g"
   },
   "outputs": [],
   "source": [
    "cs_variance = glove_v.propagate.delta_method_variance(\n",
    "    deriv_dict=deriv_dict,\n",
    "    variance_dict=variances,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "cFfFUZxkIR_R"
   },
   "outputs": [],
   "source": [
    "DM_dict = {\n",
    "    \"Method\": [\"Delta Method\"],\n",
    "    \"Mean\": [cs_pe],\n",
    "    \"Standard Deviation\": [np.sqrt(cs_variance)],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tQZx9Jq2SH6y"
   },
   "source": [
    "The **Delta Method** gives us a standard deviation of 0.010045 for the cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WEIxOYzMIR8k",
    "outputId": "54b1b40e-1be7-451a-811e-874bc1e20acf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Method': ['Delta Method'], 'Mean': [0.4432819], 'Standard Deviation': [0.010045475617181882]}\n"
     ]
    }
   ],
   "source": [
    "print(DM_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wfs2oLdwIU1o"
   },
   "source": [
    "## Method of Composition approach\n",
    "\n",
    "In this approach, we obtain $K = 100,000$ samples of the cosine similarity of these two words, using random draws from the Normal distributions of each word. We then compute an estimate for the cosine similarity and its standard deviation by looking at the mean and standard deviation over the computed samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "dtAO-Y21IR5m"
   },
   "outputs": [],
   "source": [
    "K = 100_000\n",
    "\n",
    "sample_matrix_doctor = glove_v.propagate.sample_vector(\n",
    "    variance=variances[\"doctor\"],\n",
    "    vector=vectors[\"doctor\"],\n",
    "    n=K,\n",
    ")\n",
    "\n",
    "sample_matrix_surgeon = glove_v.propagate.sample_vector(\n",
    "    variance=variances[\"surgeon\"],\n",
    "    vector=vectors[\"surgeon\"],\n",
    "    n=K,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ltWQu4iiWsle"
   },
   "source": [
    "We now compute ($Y^{(1)}, ..., Y^{(K)}$), the i.i.d. samples of the cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "oZmg0umYIZCB"
   },
   "outputs": [],
   "source": [
    "moc_cs = np.sum(sample_matrix_doctor * sample_matrix_surgeon, axis=1)\n",
    "moc_cs = moc_cs / (\n",
    "    np.linalg.norm(sample_matrix_doctor, axis=1)\n",
    "    * np.linalg.norm(sample_matrix_surgeon, axis=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "EPkOSksHIctf"
   },
   "outputs": [],
   "source": [
    "MOC_dict = {\n",
    "    \"Method\": [\"Method of Composition\"],\n",
    "    \"Mean\": [np.mean(moc_cs)],\n",
    "    \"Standard Deviation\": [np.sqrt(np.var(moc_cs))],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oba2H1V_Ic8b"
   },
   "source": [
    "## Comparison: Delta Method vs. Method of Composition\n",
    "\n",
    "We can now compare the results from the **Delta Method** and the **Method of Composition**. We see that both approaches give very similar results, with the **Delta Method** centered around the cosine similarity of the point estimates of the words and the **Method of Composition** centered around the mean of the cosine similarity samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "FtpYXoaIIgPt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Method      Mean  Standard Deviation\n",
      "0           Delta Method  0.443282            0.010045\n",
      "0  Method of Composition  0.431803            0.009874\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame.from_dict(DM_dict)\n",
    "df = pd.concat([df, pd.DataFrame.from_dict(MOC_dict)])\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-_CXHh7aIhvJ"
   },
   "source": [
    "## References\n",
    "M. A. Tanner, *Tools for Statistical Inference: Methods for the Exploration of Posterior Distributions and Likelihood Functions*, Springer Series in Statistics (Springer New York, 1996)."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
